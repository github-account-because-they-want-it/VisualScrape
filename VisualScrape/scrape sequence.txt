1- The start page.
2- The Items and their selectors and types.
3- The paging and related selectors.
4- Start Crawling.

A selector can lead to another
url to follow or an item to scrape. So this should be also specified with the selector.
If the selector leads to a url to follow or to scrape, the spider Rules are 
suitable for that. If it leads to an Item to scrape, then the selector 
shall be a list of InterestSelector objects.
Then we can start scraping. That should be done in a general way.
For each selector:
  if it was a url selector: follow the result, or append it where it can be followed
  else if it was an item selector (list of InterestSelector): return the item instead.
Of course, the scraped stuff might need input and output pipelining, and/or
processors.
Url fields may need canonicalization for example. So i'll provide some processors
for convenience and use them when absolutely necessary, like with image_urls fields
I got an idea now for making ItemPipelines work for the image_urls field
By default, the Pipeline gets passed the spider, very useless.
Since I need the response url to be able to canonicalize the image urls,
i'll make that a field on the scraped item and pop it in the pipeline.
Each selector can have a backup selector (or selectors).
The whole setup can start (and for flexibility, in between) with a login process.
Though i've never found a mid-session login process.
This is becoming a general web-automation tool.
There's a software component that collects information.
Another that uses that information to construct spider and related classes and settings.
Another class should provide the real crawling, and accept handlers for the resulting
items. Probably as the last stage in a pipeline.
What about data exporting?.
Scrapy has several exporters by default. So declaring the format and output directory
will be enough. Scrapy also allows extending these mechanisms
Also, There should be a component that abstracts obtaining results, wither from a spider or a browser.
So, how should an interaction go?
---------------------------------------------------------
1- The CrawlPath class collects 
  a- selectors
  b- selector types (url to follow or item to scrape)
  c- backup selectors, in case original selector fails
  d- Login data, if required.
2- Some kind of an (Engine) configures that CrawlPath object.
3- The engine has some choices, with priorities, of which scraper to choose.
4- The engine creates an instance of the highest priority (preference) scraper.
5- The engine passes the CrawlPath to the scraper
6- The engine configures some (item handler) object(s), and registers them with
  the scraper.
7- The engine reads about it's configured validators, creates them and passes the items to them
  when received.
8- The engine connects itself to the scraper start and stop signals, and broadcasts them to interested
   subjects when received.
8- The engine orders the scraper to start scraping.
9- The engine uses receives items, so it passes them to the validators, and any registered handlers.
10- The engine recieves the items from validators (which may modify the items), and passes them to
  the next validator, and so on.
  I think the validator can be modeled as a pipeline class. It can return back the possible
  fixed item, or None, which means it dropped the item.
11- The engine calculates a score for this scraper. Possibly with a threshold, the engine accepts or refuses
  the current scraper results.
12- If score is insufficient, the engine starts at step 4
-----------------------------------------------------------------------------------------
What do I want now?. I'm very confused.
If the Path class is Form, then I don't want to scrape any data from that page. From the following page
maybe. So I'll take the Path (whatever, i forgot)
WTF should I do now?. I want to support endless travel on this spider.
I think if the method is generic enough, it will be called from scrapy
and it shouldn't cause problems.
-----------------------------------------------------------------------------------------
How should I configure the engine?. And where should I store it?
I think it can be exported to the parent area. Or maybe not.
-----------------------------------------------------------------------------------------
The spider logic is quite broken now. First, the URL extraction is done out
of sequence. This might not be a problem, but now I also don't know when to
declare a path traveled.
Every selector can result in several requests. I mean the url selectors here. 
Probably I should have a couple of parse methods. One for urls, another for items?
I think it can go like that for the Scrapy spider:
It's the same as now, but I'll try all selectors on all retrieved pages, and not just the
pages they were intended to filter. But this might find accidental items, and will complicate
filtering.
So what should I do?
Each site-selectors combination should have it's own parser method.
This way, i can differentiate between paths.
That's good and I can do it. But I think my spider path scheme is fucked.
Or at least I don't know how to use it!. Or maybe i do
I think that the Path idea was wrong.
The spider path should be setup like this:
1- Add a start destination (url to follow, form to post)
2- Add a selector (item selector or url selector) and you know what to do with these
3- Add another selector and so on...
4- A url selector may declare that it's a termination url and thus it's 
   results shouldn't be scraped. And how should I continue after that?.
---------------------------------------------------------------------------------------
So, what's the spider algorithm?
Now I want to fuckin' connect the spider signals to their handlers
The handlers can come relatively easily from callers. But how can I connect the
damn spider signals to them?. The spider doesn't have access to it's own signals it
seems
---------------------------------------------------------------------------------------
Now, I want to attach a favicon field to the first scraped item. I may make a special
request method for it, instead of tracking it the whole time in he main scraping method.
How can I make the first item that special?. I can add it in the pipeline, but
that won't make it automatically downloaded by Scrapy
---------------------------------------------------------------------------------------
Now that scrapy selectors are different from selenium, and they'll even raise
exceptions if used, I need a conversion step to the selectors, before they can be used
with selenium webdriver.
What can I do. I'm lost.
First, the itemPageSelector. This is obviously a url selector, so the converter
should take it and the browser, strip off the extra parts, probably canonicalize,
and return it to the browser.
Ok, that's good for link selectors.
We've got also the text ones. And now the converter should take the selector, and
actually return the required text.
--------------------------------------------------------------------------------------
Look, boss. Using browser.get() on some links will only send you to the same page again.
So, it's obvious, you'll need to associate actions with elements. Or, take the PROBABLY safer
route, and return elements that will be clicked instead of urls that are visited.
And not just this...
There are now clickable divs!. And visible elements that are claimed invisible by Selenium
and thus cannot be clicked.
So I've used the scrapy link extractor to get me some links to visit. That was great, but it won't 
work for clickable divs.
Ok. So, basically now, I need a new way to extract "Item Links" and another way
to indicate what to do with the link. Click it or browser.get it's url.
Ok. So what I know now is 2 kinds of links. Those that should be clicked and waited
and those kind ones with href that can be followed.
So probably I should ignore link extractors for selenium. Selenium will be launched for
js-heavy sites, which mostly consider the href attribute old-fashioned.
Ok. So I think I should consider both straightforward href and clickables (spans, divs..)
in my account.
I can still use the link extractor with hrefs and mouse clicks and waits for the others. ENOUGH.
Finally, Use selectors for both item keys and item values. This will be a guard
against missing rows, especially when working with mongodb, this will be a boon.
And now I need to handle link extraction. As I said, 2 kinds of links. And 2 kinds of extractors.
A regexp extractor and a "selector" extractor. The selector extractor is just a FieldSelector.
---------------------------------------------------------------------------------------
Maybe I'm not in the mood of coding, given my headache. There should be a component that measures
or analyzes the output from a spider, interrupting the engine when the results are "bad".
Though the engine has no control of the spider, it has access to the manager. That should be
enough.
Reasons for the switch:
1- Identical response pages (!)
2- Empty items (that doesn't indicate a bad spider. It denotes a probably bad selectors.
I think the switch idea is really stupid. Any crawling process knows beforehand whether 
or not a browser should be used.
But I think a component could have access responses and warn when identical pages are retrived.
Not very useful.
So, I think the switching is bad idea. Surprise, yes, because that's the main idea behind
the program in the first place.
---------------------------------------------------------------------------------------
Now that some sites require special handling for their fields, that opens space for
2 options. Either item loaders or pipelines can be used for that. It also complicates
the future automation process.
Since I allow spaces into field names, and that item loaders operate by field names,
this forces me to use dynamically created item loaders, or item loaders that customize the 
__new__ method for the item loader. 
---------------------------------------------------------------------------------------
I'm being literally fucked by the settings. What should I do. Best, I want a general solution
for nested settings.
Let's return to out problem here. I want settings to be fault tolerant and return nice 
defaults.
I think the idea of a subclass for every specific setting can be good.
---------------------------------------------------------------------------------------
It shouldn't be that fucking hard. But changing item field names is not easy it seems.
